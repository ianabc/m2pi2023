{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "Pandas (Python Data Analysis Library) is an extremely popular module which you'll find at the top of a huge proportion of data science notebooks. Like numpy, it's also popular enough to deserve its own import idiom..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick idea of the scope of pandas take a look at the autocomplete for pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental objects in pandas are the `Series` and the `DataFrame`. These build on the idea of a numpy `ndarray`, but they add the idea of an index and they include a rich set of methods which we can use to manipulate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "The pandas `Series` object is basically a one dimensional _indexed_ array. Schematically, they look like\n",
    "\n",
    "| Index | Value |\n",
    "|-------|-------|\n",
    "| 0     |  0.12 |\n",
    "| 1     |  0.24 |\n",
    "| 2     |  0.36 |\n",
    "| 3     |  0.48 |\n",
    "\n",
    "There are two columns: an index and a value. Most of the time the index values are distinct (this isn't a firm requirement though!) but the index doesn't have to be integers. Any hashable type will do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series({'one' : 1.0, 'two': 2.0, 'three': 3.0})\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly common option is to use a timestamp as the index (don't worry about the syntax here, we'll come back to time-series later)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dti = pd.date_range('2020-06-03', periods=3, freq='H')\n",
    "pd.Series(['first', 'second', 'third'], index=dti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to notice is that the values all share the same type (e.g. `dtype: float64` for `s1`, `object` for `dti`). `pandas` will make `Series` of almost any type as long as all the rows share the same type, but it will always try to pick the most efficient implementation. For numerical values, it'll use `numpy.ndarrays`, but if it can't infer a specific type, it will fall back to a generic `object` type. It really pays (in speed) to keep an eye on the `dtype` and use a `numpy.ndarray` type where possible. For categorical values there is a special `Categorical` `dtype` which can help when interfacing with `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making `Series`\n",
    "\n",
    "You'll probably find that you want to deal with `DataFrames` more often than `Series`, but lots of operations in `panadas` return Series (or want them as an argument in some operation) so it's good to know how to build them.\n",
    "\n",
    "Like `ndarray`'s, you can initialize a `Series` from a sequence, by default it'll get an ascending integer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.Series([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = pd.Series(range(5))\n",
    "si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this looks a lot like a numpy array (or even just a list), but we can switch the indexing to suit our needs, by explicitly passing the `index=` arguement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = pd.Series([1., 2., 3., 4., 5.], index=['one', 'two', 'three', 'four', 'five'])\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "\n",
    "square bracket notation will select by index value, this is convenient, but see `.loc` and `.iloc` later for more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm['three']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the series is starting to look more like a dictionary, in fact, that's another good way to construct `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = pd.Series({'one': 1, 'three': 3, 'two': 2, 'four': 4, 'five': 5})\n",
    "sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look closely though, a Series has a few tricks that a dictionary doesn't..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn['three':'four']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. label based indexes are _inclusive_ of the `stop` value. This is different from most other indexes you'll see in python, and can cause a little confusion. If you use numbers to do the slice, you'll get the familiar python behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series have a `keys()` method, but it returns an index object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element by element statements evaluate to Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if you remember the material on numpy fancy indexing, this can be very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn[sn > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine these conditions in more complicated expressions, but be careful that `&` isn't interpretred as a bitwise operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn[(sn > 2) & (sn < 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some extra indexing methods available to you: `.loc`, `.iloc`, `.ix`. These let you be much more explicit about exactly what you want to return. At a very basic level, they can help you avoid label confusion..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sa = pd.Series(data=['apple', 'banana', 'orange', 'pineapple'], index=[3, 2, 1, 7])\n",
    "\n",
    "sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want the value in the 4th row..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, `3` was interperted as a label. There's an ambiguity because of the type of the index. Fortunately `pandas` gives us a pair of functions to avoid that problem\n",
    "\n",
    "  * `.iloc[]`: Purely integer-location based indexing.\n",
    "  * `loc[]` : Purely label based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `.iloc` will also work with slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[:3:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical and fancy indexing work here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[[2, 1, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical indexing works with these methods as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.loc[sn>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Exercise**: Create a series with the letters of the alphabet as values (try using `enumerate`). Use the .`sort_index` method to reverse the `Series` and slice the result to pick out the values between `t` and `k` (inclusive)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "Most of the time you will be using `DataFrames` rather than `Series` (though the result of many `DataFrame` operations is a `Series`), but at a first pass it is OK to think of `DataFrames` as a bunch of `Series` stuck together with a common index. \n",
    "\n",
    "\n",
    "| Index | Value1 | Value2 | \n",
    "|-------|--------|--------|\n",
    "|   0   |  0.12  |   'a'  |\n",
    "|   1   |  0.24  |   'b'  |\n",
    "|   2   |  0.36  |   'c'  |\n",
    "|   3   |  0.48  |   'd'  |\n",
    "\n",
    "The rules discussed above for the index stay the same but notice that we can now have different types in the various columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame({'floats': sm, 'ints': sn})\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Attributes\n",
    "\n",
    "We've already seen some of the attributes of the DataFrame (column etc.) but there are quite a few available, take a look at `index`, `columns`, `shape`, `dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some useful utility functions for getting oriented with the contents of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you pass `include=all` to describe, it will also try to tell you what it can about non-numerical values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "When indexing a dataframe, the default is to give you the column (you can also use the syntax `d1.ints`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['ints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking for the row, then try `.loc` with the row index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc['one']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, you probably want to lean towards `.loc` when you want to subset rows and/or columns. It is much more flexible and helps pandas resolve some potential ambiguities about what you want to select. Here is a column selection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc['four':, ['ints']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The square brackets around 'ints' here tell pandas I want a `DataFrame` to be returned, rather than a `Series`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `.iloc will interprete both row and column specifiers as numerical positions\n",
    "d1.iloc[:2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical/fancy indexing works with `.loc` and `.iloc` as well, but remember the parentheses or `&` will be interpreted as a bitwise operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc[(d1['ints'] > 2) & (d1['floats'] < 4), ['ints']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc[d1.ints.isin((1, 4))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wherever possible, `pandas` (like `numpy`) will try to return a view on the same data rather than a copy, but because the indexing possibilities in `pandas` are much greater, the specific rules are more subtle, suffice to say, `.loc` and `.iloc` will generally help resolve the ambiguity and if you really need to break the link between two dataframes you can use the `.copy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1a = d1\n",
    "d1a is d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1b = d1.copy()\n",
    "d1b is d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames\n",
    "\n",
    "`DataFrames` are mutable; we can change the values in rows and columns and we can add/remove columns in place. `pandas` will usually operate in place, but some modifications (e.g. changing column dtype) require implicit data copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.iloc[1] = (3.0, 3)\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['ratio'] = d1['ints'] / (2 * d1['floats'])\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Patching up\" dataframes by modifying individual cells or groups of cells can be a little bit tricky. The basic reason is that a lot of the operations you would like to use to address the cell will return a copy of the dataframe but in general you want to modify the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['floats']['five'] = 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of this issue is described in the warning you'll see given above, but if you only need to change a single (scalar) value you can use `pd.DataFrame.at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.at['five', 'floats'] = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more general cases you can use `pd.DataFrame.loc`, (N.B. Avoid chained indexing ([][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.loc[:, 'floats'] += 0.1\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` also has facilities like `fillna` or `replace` to fill in multiple missing values using various strategies, we'll look at those later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Methods\n",
    "\n",
    "There are *lots* of methods for operating on DataFrames, have a look at the tab completion and explore the documentation for them. In particular, take a look at the help for `describe`, `head` and `tail`. These are great for orienting yourself with a new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have time to dive into all of the methods but we'll sample a few, then you should explore the documentation for others. Some of the most often used are\n",
    "\n",
    "* `min`, `max`\n",
    "* `mean`, `mode`, `median`\n",
    "* `max`, `min`, `idxmax`, `idxmin`\n",
    "* `any`, `all`\n",
    "* `astype`\n",
    "* `dropna`\n",
    "* `sort_index`, `sort_values`\n",
    "* `plot`\n",
    "\n",
    "Generally these will return another `DataFrame` with the results you are looking for, but you can also pass the `inplace=True` keyword argument which will modify the `DataFrame` in place and save some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d1 > 3).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can coerce values to different type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['floats'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sort values (or indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.sort_values('floats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Exercise**: Calculate the mean value of the floats column and use it to calculate how far each float is from the mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dropna` method can come in handy sometimes. In general, pandas is very good at [handling missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html), but in some other modules will choke when they encounter a `np.NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.at['three', 'floats'] = np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operating directly on the numpy array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['floats'].values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operating through pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['floats'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1['floats'].dropna().values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also convenience methods like `.fillna` for forward or backward filling missing values etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with External Data\n",
    "\n",
    "Data comes in many forms from simple csv/json files, real-time APIs, structured binary files and many others. Try `pd.read_<TAB>` to see some of the `pandas` ingestion options. `read_csv` is the main workhorse for data sets which will fit on a single machine. It is way more flexible than it's name suggests (S3 buckets, https, compressed files, ...) and many of the arguments to `read_csv` will have equivalents for the other functions, so we'll start by looking at it.\n",
    "    \n",
    "We need a CSV to work with. The city of Vancouver has an [open data catalog](https://vancouver.ca/your-government/open-data-catalogue.aspx), which has CSV for some of the datasets. There's a dataset which lists all of the [community gardens and food trees](https://opendata.vancouver.ca/explore/dataset/community-gardens-and-food-trees/information/) maintained by the city. A copy of it is available in this directory called `CommunityGardensAndFoodTrees.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF = pd.read_csv(\n",
    "    \"https://opendata.vancouver.ca/explore/dataset/community-gardens-and-food-trees/download/?format=csv&timezone=America/Los_Angeles&lang=en&use_labels_for_header=true&csv_separator=%3B\",\n",
    "    delimiter=';'\n",
    ")\n",
    "gardenDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.describe(include='all', datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 168 rows, with 20 columns, here are the fist few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can tell things like the gardens were created (`YEAR_CREATED`), and where the are (`LATITUDE`, `LONGITUDE`), and who's responsible for them (`STEWARD_OR_MANAGING_ORGANIZATION`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to clean the data. This is a hugely important step and will generally eat a lot of your time, but it is worth doing right. Having mistakes in your data can undermine everything you subsequently try to infer from it.\n",
    "\n",
    "First let's look at the index, the default is to index by integer, but we could have picked any column instead. It looks like the first column is unique (`MAPID`) and so let's use that (chosing the index right can make your life much easier when adding data or combining multiple DataFrames). The `inplace=True` argument means modify the existing dataframe rather than returning a modified copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.set_index('MAPID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful trick when cleaning data is to look at the unique values in a column. You'll often catch coding mistakes or values being used as placeholders this way, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value counts can also be useful to spot outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['Geo Local Area'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `YEAR_CREATED`, `Pre-2010`, `pre-1970` and `pre 2000` are kind of usless (and inconsistent!) so let's toss them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for badLabel in ['Pre-2010', 'pre-1970', 'pre 2000']:\n",
    "    gardenDF = gardenDF.loc[gardenDF['YEAR_CREATED'] != badLabel]\n",
    "\n",
    "gardenDF['YEAR_CREATED'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still an `np.NaN`, and the years are strings (numbers would be better, or even dates). We can take a closer look at the rows which are missing their `YEAR_CREATED` using isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.loc[gardenDF['YEAR_CREATED'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has some facilities for [dealing with missing data](https://pandas.pydata.org/docs/user_guide/missing_data.html) as well as some helpful default behaviours. Additionally, the `.isna` and `.notna` methods can help you explicitly deal with missing values during operations, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingDF = pd.DataFrame(\n",
    "    np.random.randn(4, 3),\n",
    "    index = ['one', 'two', 'three', 'four'],\n",
    "    columns = ['a', 'b', 'c']\n",
    ")\n",
    "missingDF['d'] = list(np.random.randn(2)) + [np.nan] + [np.random.randn()]\n",
    "missingDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Exercise**:In some cases you might want to patch up missing data by combining one DataFrame with another, or by interpolating from nearby values_\n",
    "\n",
    "  * _Use `.fillna` to fill with the value from the row above and/or below_\n",
    "  * _Fill with the mean of the other column values_\n",
    "  * _Fill with the mean of the other row values (`NotImplemented`, can `.T` help?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to our example, for cases where that isn't possible (like here, where I'm too lazy), you can also just throw away the rows without valid years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF.dropna(axis=0, subset=['YEAR_CREATED'], inplace=True)\n",
    "gardenDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it might make sense to convert that column to a numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(gardenDF['YEAR_CREATED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or better yet, a DateTime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'] = pd.to_datetime(gardenDF['YEAR_CREATED'])\n",
    "gardenDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you are not so lucky and will have to parse through strings to extract the information you are looking for. The `Geom` column looks like latitude and longitude stored as json, lets parse an individual entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "geom = gardenDF.iloc[-1]['Geom']\n",
    "json.loads(geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a `dict` and `coordinates` holds the longitude and latitude. We can use `pd.DataFrame.apply` to apply this transformation to all of the rows and generate new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['Longitude'] = gardenDF['Geom'].apply(lambda x: json.loads(x)['coordinates'][0])\n",
    "gardenDF['Latitude']  = gardenDF['Geom'].apply(lambda x: json.loads(x)['coordinates'][1])\n",
    "gardenDF[['Latitude', 'Longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have an idea of what your data source looks like there are some arguments to `read_csv` (and related functions) which can tidy things up as they are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def geom2lat(geom):\n",
    "    if geom:\n",
    "        return json.loads(geom)['coordinates'][0]\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "    \n",
    "gardenDF = pd.read_csv(\n",
    "    \"https://opendata.vancouver.ca/explore/dataset/community-gardens-and-food-trees/download/?format=csv&timezone=America/Los_Angeles&lang=en&use_labels_for_header=true&csv_separator=%3B\",\n",
    "    usecols = [\n",
    "        'MAPID',\n",
    "        'YEAR_CREATED',\n",
    "        'NAME',\n",
    "        'STEWARD_OR_MANAGING_ORGANIZATION',\n",
    "        'STREET_NUMBER',\n",
    "        'STREET_NAME',\n",
    "        'Geom',\n",
    "        'Geom'\n",
    "    ],\n",
    "    delimiter=';',\n",
    "    encoding='latin1',\n",
    "    na_values={\n",
    "        'YEAR_CREATED': ['Pre-2010', 'pre-1970', 'pre 2000', 'nan']\n",
    "    },\n",
    "    index_col='MAPID',\n",
    "    parse_dates=['YEAR_CREATED'],\n",
    "    converters={\n",
    "        'Geom': geom2lat,\n",
    "    }\n",
    ")\n",
    "gardenDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data via APIs\n",
    "\n",
    "An API (Application Programming Interface) is a formal specification describing how systems should communicate with each other. This is an incredibly general notion which could involve information going in both directions and changes being made on both systems. We will only talk about them in the context of extracting data and massaging it into pandas, but full featured APIs could also support authentication, uploading information, telling AWS to shutdown all your ec2 instances or pretty much anything else you could think of.\n",
    "\n",
    "APIs allow applications (your code) to interact with other applications (someone elses' code) and they're particularly useful where the incoming data is event based or otherwise frequently updated. They also turn up in situations where the entire data set is very large, but individual requests will only need small slices; applications can request the information they need on demand without the source having to pre-prepare all the possible variations for download.\n",
    "\n",
    "Good APIs are versioned (or at the very least stable), well documented and they often implement a pattern called [REST](https://en.wikipedia.org/wiki/Representational_state_transfer). This adds a layer of formality and standardization to the API which which helps to make them more predictable and easier to develop against. Most of the API's I can think of operate over HTTP and pass information back and forth in the [json](https://en.wikipedia.org/wiki/JSON) format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For popular APIs (e.g. [twitter](https://developer.twitter.com/en/docs/twitter-api)) you might find that someone has already written a python wrapper for the API (e.g [python-twitter](https://python-twitter.readthedocs.io/en/latest/)). This is usually the best case scenario because it means someone else has done the hard work of reading the API specification for you. In other cases, you might only have access to the API spec or maybe a console to help you build queries. If you're very unlucky, the API will be undocumented, the silver lining in this case is these are usually where the really juicy stuff is stored!\n",
    "\n",
    "* [Vancouver Open Data Portal](https://opendata.vancouver.ca/api/v1/console/datasets/1.0/search/) has a console to help you build queries\n",
    "* [Canada Open Data Portal](https://open.canada.ca/en/working-data#toc93c) has instructions on interfacing with python\n",
    "* [Stats Canada API](https://www.statcan.gc.ca/eng/developers/wds/user-guide#a12-3) See also [this post](https://towardsdatascience.com/how-to-collect-data-from-statistics-canada-using-python-db8a81ce6475) on using that portal\n",
    "* [Twitter API](https://developer.twitter.com/en/docs)\n",
    "* [Open Weather Map](https://openweathermap.org/api)\n",
    "* [NASA](https://api.nasa.gov/) imagery, data, etc.\n",
    "* [GitHub](https://docs.github.com/en/rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there isn't a pre-packaged module to take care of your needs the [requests module](https://docs.python-requests.org/en/master/) is usually your best option. Here is a quick example extracting the part of the gardens dataset from the vancouver data portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from urllib.parse import urlencode, urljoin\n",
    "\n",
    "base_url = 'https://opendata.vancouver.ca/api/v2/'\n",
    "catalog_item = 'catalog/datasets/council-voting-records/records'\n",
    "headers = {\n",
    " 'Content-Type': 'application/json; charset=utf-8'\n",
    "}\n",
    "\n",
    "\n",
    "params = {\n",
    "    'dataset_id' : 'council-voting-records',\n",
    "    'limit'      : 10,\n",
    "    'where'      : 'meeting_type = \"Council\"',\n",
    "}\n",
    "\n",
    "\n",
    "r = requests.get(urljoin(base_url, catalog_item) , params=params)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look up these params in the the [API documentation](https://opendata.vancouver.ca/api/v2/console#!/dataset/getRecords) you'll see that we are filtering for records where `meeting_type` is `Council` and we are asking for a `limit` of 10 records (which is actually the default value of `limit`). But there are more records in total which match that filter (look at the first line of `r.json()`. If we want them all, we have to access them in pages. Before we do that though, we should think about rate limits. Almost every API will implement some form of rate limit and the Vancouver Open Data (as well as a lot of other places) will tell you what your current rate limit status is in the response headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k, v in dict(r.headers).items() if k.startswith('X-Rate')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Their documentation](https://help.opendatasoft.com/apis/ods-search-v1/#quotas) has information on these values. In general, registered users will be given more generous limits. Often this means registering for some kind of token and including it in your requests, we will do this below, but with one **major** caveat. The token is usually equivalent to a password and you should treat it the same way. Don't share it with people, be careful where you use it (including jupyter notebook output cells!), and make sure you don't check it into version control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['VAN_API_KEY'] = getpass('API Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['apikey'] = os.environ['VAN_API_KEY']\n",
    "r = requests.get(base_url, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k, v in dict(r.headers).items() if k.startswith('X-Rate')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new rate limit we can make multiple requests and get all of the data we wanted. Even so it is worth limiting the impact we will have on their API, there are modules like [ratelimit](https://pypi.org/project/ratelimit/) which can do this systematically, but we will just add a manual delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from urllib.parse import urlencode, urljoin\n",
    "\n",
    "records = []\n",
    "params['rows'] = 50\n",
    "\n",
    "url  = f\"{urljoin(base_url, catalog_item)}?{urlencode(params)}\"\n",
    "json = ''\n",
    "while len(records) < 250:\n",
    "    time.sleep(2)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    urls = {x['rel']:x['href'] for x in r.json()['links']}\n",
    "\n",
    "    records.extend(r.json()['records'])\n",
    "    \n",
    "    if urls['self'] == urls['last']:\n",
    "        break\n",
    "    else:\n",
    "        url = urls['next']\n",
    "        params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of these json objects, the important stuff is in the 'record' key, under 'fields'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = pd.DataFrame([chunk['record']['fields'] for chunk in records]).set_index('vote_detail_id')\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[votes[col] = votes[' for col in ['meeting_type', 'vote', 'decision']]\n",
    "votes = votes.astype({\n",
    "    'meeting_type': 'category',\n",
    "    'vote': 'category',\n",
    "    'decision': 'category',\n",
    "})\n",
    "votes['vote_start_date_time'] = pd.to_datetime(votes.vote_start_date_time)\n",
    "votes['vote_date'] = pd.to_datetime(votes.vote_date)\n",
    "votes.decision.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries\n",
    "\n",
    "We've already talked a bit about time and date handling, but the author of pandas wrote it to deal with time series data so it really excels here. The main objects to be aware of are\n",
    "\n",
    "  * **Time Stamps**: Specific points in time usually recorded to the second or nanosecond\n",
    "  * **Time Intervals/Time Deltas**: These types lets you do arithmetic on time objects\n",
    "\n",
    "and there associated indices. We need some dates to play with. There's a convenience function called `to_datetime` which can convert many \"human readable\" dates to a pd.Timestamp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "moonwalk = pd.to_datetime('July 20, 1969, 20:17 UTC')\n",
    "moonwalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamps have attributes which let you extract days, year, etc. Normally these will be reported as numbers, but the strftime method supports the usual format specifiers (The correspond with the libc specifiers, here's a reference http://strftime.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonwalk.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The moon walk took place on a {moonwalk.strftime('%A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(datetime.utcnow(), utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another sample dataset. This time, it is a record of historical flight data. It contains various columns, we will look at\n",
    "\n",
    "  * `activity_period`: The date for the record in the format yyyymm\n",
    "  * `passenger_count`\n",
    "  \n",
    "We can parse the date into a DateTime with `pd.to_datetime` then set the result as the index. We can then slice dates and resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://data.sfgov.org/resource/rkru-6vcg?$limit=10000')\n",
    "\n",
    "flightsDF = pd.DataFrame(r.json())\n",
    "flightsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF['passenger_count'] = flightsDF['passenger_count'].astype(int)\n",
    "flightsDF['date'] = pd.to_datetime(flightsDF['activity_period'], format='%Y%m')\n",
    "\n",
    "flightsDF.set_index('date', inplace=True)\n",
    "flightsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyFlightsDF = flightsDF[['passenger_count']].resample('M').sum()\n",
    "monthlyFlightsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyFlightsDF.loc['2021-01-01':, 'passenger_count'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamp objects can also deal with arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.index[-1] - flightsDF.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF['passenger_count'].resample('Y').mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the `value_counts` method can be very useful to get an idea of how categorical values are distributed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF['geo_region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of the algorithms in scikit learn, we need to convert categorical variables into numerical values. The simplest case is [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) which Pandas can do this for us with the `.get_dummies` method (sklearn also has it's own methods, but it's handy to be able to do this in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_one_hot = pd.get_dummies(flightsDF['operating_airline_iata_code'])\n",
    "flightsDF = flightsDF.drop([\n",
    "    'operating_airline_iata_code',\n",
    "    'operating_airline',\n",
    "    'published_airline',\n",
    "    'published_airline_iata_code',\n",
    "], axis = 1\n",
    "                          )\n",
    "flightsDF.join(airlines_one_hot).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Exercise**: Try resampling to find the total passenger count each quarter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping, Joining, Concatenating\n",
    "\n",
    "When working with a new dataset, I recommend trying to stuff as much as you can into a single `DataFrame` to try to help find the basic patterns, but sooner or later you will want to do aggregate operations within a `DataFrame` (e.g. group together all of the rows by year and show the mean value of some other column) or combine two or more `DataFrame`s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has some old information about car engine efficency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv'\n",
    ")\n",
    "carsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Let's group things by `number_of_cylinders` and see how that affects mpg..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF['mpg'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `groupby` on it's own will give you a `DataFrameGroupBy` object. Informally you can think of this as an iterator for \"sub\"-`Dataframe`s or slices of your original `DataFrame`. You're expected to perform aggregate operations (e.g. `sum`, `mean` or generic `.apply` methods) on these groups to actually see some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders = carsDF.groupby('cylinders')\n",
    "carsDFbyCylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical aggregate operations are things like\n",
    "\n",
    "  * mean()\n",
    "  * sum()\n",
    "  * median()\n",
    "  * min()/max()\n",
    "  * count()\n",
    "  * nunique()\n",
    "  * size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agg` method lets you apply arbitrary aggregate functions as well, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders['mpg'].agg(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Exercise**: Try grouping my model year and looking at the median value for each group_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by `groupby` is an iterator, so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vroom, group in carsDFbyCylinders:\n",
    "    print(f\"There are {group.shape[0]} cars with {vroom} cylinders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are there possibly 4 cars with 3 cylinders?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.get_group(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply multiple operations at the same time. The `.agg()` method can take a list of the operations you want to perform (e.g. [\"max\", \"min\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders['mpg'].agg([\"min\",\"max\",\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter based on our groups. This isn't the recommended way of doing the following, but let's manually group the cars by cylinder count, then find any particular cars that are more than 1.5 times the mean mpg of their group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF[\n",
    "    carsDFbyCylinders.apply(lambda x: x.mpg > 1.5 * x.mpg.mean()).reset_index(level=0).mpg\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's actually a transform method lets you perform a group operation then use the results to update the rows. For example, we could calculate mean values for our groups, then look at how individual cars perform relative to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = carsDFbyCylinders.mpg.transform('mean')\n",
    "carsDF[carsDF.mpg > 1.5 * means]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look at the best and worst performing car relative to the group means, `idxmin` and `idxmax` are to pandas what `argmin` and `argmax` are to numpy (see also `nlargest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF['mpg_mean'] = carsDF['mpg'] - means\n",
    "carsDF.loc[carsDF.groupby('cylinders')['mpg_mean'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF.loc[carsDF.groupby('cylinders')['mpg_mean'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also group by multiple conditions, but you'll get a hierarchical index as the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF.groupby(['origin', 'cylinders']).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't cover Hierarchical indices, so for now we will just flatten the result by resetting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDF.groupby(['origin', 'cylinders'])['mpg'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. `.transform` and `.apply` exist as methods of both ordinary DataFrames and `DataFrameGroupBy` objects, depending on what you want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate & Join\n",
    "\n",
    "There are a handful of functions which handle concatenation. The main ones are `pd.concat`, `pd.join` and `pd.merge` and there is some overlap in exactly what they do so. You can combine either `Series` and `DataFrames` but we'll jump straight to `DataFrames`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.DataFrame(\n",
    "    {\n",
    "        'upper': ['A', 'B', 'C'], \n",
    "        'lower': ['a', 'b', 'c']\n",
    "    }, \n",
    "    columns=['upper', 'lower'], \n",
    "    index=[1,2,3]\n",
    ")\n",
    "\n",
    "s2 = pd.DataFrame(\n",
    "    {\n",
    "        'upper': ['D', 'E', 'F'],\n",
    "        'lower': ['d', 'e', 'f']\n",
    "    }, \n",
    "    columns=['upper', 'lower'],\n",
    "    index=[1, 2, 3]\n",
    ")\n",
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.join` will combine by aligning on the index, but expects unique column names, we can use the suffix keyword to make our columns unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.join(s2, lsuffix='_l', rsuffix='_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With concat we can combine rows (we'll reindex to get unique index values in the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1,s2]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we enclosed the things we want to join as some sort of iterable (a `list` here).\n",
    "\n",
    "`concat` can also combine columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.concat` will accept duplicate indices, but normally that indicates a problem with the data normalization. `concat` has a `verify_index` argument which can check for these problems and you can specify what you want to do with duplicates manually.\n",
    "\n",
    "concat will often result in `NaN`s because some columns might not exist in both/all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = pd.DataFrame({'fruit': ['apple', 'orange'], 'veg': ['brocolli', 'carrot'], 'tree': ['cedar', 'alder']})\n",
    "d5 = pd.DataFrame({'veg': ['onion', 'potato'], 'fruit': ['banana','grape']})\n",
    "pd.concat([d4,d5], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the index wasn't important here, I threw it away and just accepted a new one.\n",
    "\n",
    "In the general case joining DataFrames can get complex. The concat method can take a `join` keyword to specify a database like join stragegy (inner or outer), but `pd.merge` is a bit more flexible. It implements the usual relations\n",
    "\n",
    "  * one-to-one (similar to a concat)\n",
    "  * many-to-one\n",
    "  * many-to-many\n",
    "\n",
    "and lets us join based on column value(s). Here is an example of a many-to-one merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf1=pd.DataFrame({\n",
    "    'class': ['insect', 'spider'], \n",
    "    'legs': [6, 8]}\n",
    ")\n",
    "\n",
    "adf2=pd.DataFrame({\n",
    "    'name': ['molly', 'anna', 'stephen', 'mica'], \n",
    "    'class': ['insect','insect','spider','insect']}\n",
    ")                     \n",
    "\n",
    "print(adf1); print(adf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(adf1, adf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _many-to-one_ is many different rows in adf2 being mapped to a single row in adf1 (insects). `pd.merge` also accepts a selection of keyword arguments so you can manually specify which columns to join, patch up name differences etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a dataset on UFO observations for the US and Canada, read it in and try to extract the following information\n",
    "\n",
    "* _How many from each country?_\n",
    "* _States with the most observations_\n",
    "* _What is the most common shape observed in each country/state?_\n",
    "* _Find the most recent report from each province in canada_\n",
    "* _Look at the number of observations in California by month_\n",
    "* _For each state find the ufo hotspot with a `.pivot table`_\n",
    "  * _Since _lat_ and _lng_ are just coordinates, try plotting the hot spots_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufoDF = pd.read_csv(\n",
    "    'https://m2pi.syzygy.ca/data/UFOs_coord.csv', \n",
    "    names = ['Date', 'Country', 'City', 'State', 'Shape', 'Summary', 'lat', 'lng'],\n",
    "    skiprows=[0],\n",
    "    encoding='latin1'\n",
    ")\n",
    "ufoDF['Date'] = pd.to_datetime(ufoDF.Date.str.replace('.', ':', regex=False))\n",
    "ufoDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other bits and pieces...\n",
    "\n",
    "* pd.cut: Binning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.random.default_rng(42)\n",
    "age = g.integers(15, 70, size=15)\n",
    "bins = [10, 19, 20, 29, 30, 45, 55, 70]\n",
    "age_groups = pd.cut(age, bins)\n",
    "type(age_groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
